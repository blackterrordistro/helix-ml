//! üåÄ HelixML Mixed Precision Example
//! 
//! –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è mixed precision (FP16/INT8) —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏.

use helix_ml::*;
use helix_ml::tensor::{TensorRandom, TensorMixedPrecision, TensorOps};

fn main() -> Result<()> {
    println!("üåÄ HelixML Mixed Precision Example");
    println!("==================================");
    
    let device = Device::cpu();
    
    // 1. –°–æ–∑–¥–∞–µ–º —Ç–µ–Ω–∑–æ—Ä –≤ FP32
    println!("\n1. Creating FP32 Tensor:");
    let tensor_f32 = CpuTensor::random_uniform(Shape::new(vec![3, 4]), -1.0, 1.0, &device)?;
    println!("  Original tensor dtype: {:?}", tensor_f32.dtype());
    println!("  Original data: {:?}", tensor_f32.data());
    
    // 2. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ FP16
    println!("\n2. Converting to FP16:");
    let tensor_f16 = tensor_f32.to_f16()?;
    println!("  FP16 tensor dtype: {:?}", tensor_f16.dtype());
    println!("  FP16 data: {:?}", tensor_f16.data());
    
    // 3. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –æ–±—Ä–∞—Ç–Ω–æ –≤ FP32
    println!("\n3. Converting back to FP32:");
    let tensor_f32_restored = tensor_f16.to_f32()?;
    println!("  Restored FP32 tensor dtype: {:?}", tensor_f32_restored.dtype());
    println!("  Restored data: {:?}", tensor_f32_restored.data());
    
    // 4. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ INT8
    println!("\n4. Converting to INT8:");
    let tensor_i8 = tensor_f32.to_i8()?;
    println!("  INT8 tensor dtype: {:?}", tensor_i8.dtype());
    println!("  INT8 data: {:?}", tensor_i8.data());
    
    // 5. –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ INT32
    println!("\n5. Converting to INT32:");
    let tensor_i32 = tensor_f32.to_i32()?;
    println!("  INT32 tensor dtype: {:?}", tensor_i32.dtype());
    println!("  INT32 data: {:?}", tensor_i32.data());
    
    // 6. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è quantization
    println!("\n6. INT8 Quantization:");
    let scale = 0.1;
    let zero_point = 0;
    
    let quantized = tensor_f32.quantize_int8(scale, zero_point)?;
    println!("  Quantized dtype: {:?}", quantized.dtype());
    println!("  Quantized data: {:?}", quantized.data());
    
    // 7. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è dequantization
    println!("\n7. INT8 Dequantization:");
    let dequantized = quantized.dequantize_int8(scale, zero_point)?;
    println!("  Dequantized dtype: {:?}", dequantized.dtype());
    println!("  Dequantized data: {:?}", dequantized.data());
    
    // 8. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ç–æ—á–Ω–æ—Å—Ç–∏
    println!("\n8. Precision Comparison:");
    let original_max: f32 = tensor_f32.data().iter().fold(0.0, |acc, &x| acc.max(x.abs()));
    let restored_max: f32 = tensor_f32_restored.data().iter().fold(0.0, |acc, &x| acc.max(x.abs()));
    let dequantized_max: f32 = dequantized.data().iter().fold(0.0, |acc, &x| acc.max(x.abs()));
    
    println!("  Original max value: {:.6}", original_max);
    println!("  FP16->FP32 max value: {:.6}", restored_max);
    println!("  INT8->FP32 max value: {:.6}", dequantized_max);
    
    // 9. –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —Å Linear —Å–ª–æ–µ–º
    println!("\n9. Mixed Precision with Linear Layer:");
    
    // –°–æ–∑–¥–∞–µ–º Linear —Å–ª–æ–π
    let linear = Linear::<CpuTensor>::new(4, 3, &device)?;
    let input = CpuTensor::random_uniform(Shape::new(vec![2, 4]), -1.0, 1.0, &device)?;
    
    println!("  Input dtype: {:?}", input.dtype());
    
    // Forward pass –≤ FP32
    let output_f32 = linear.forward(&input)?;
    println!("  Output FP32 dtype: {:?}", output_f32.dtype());
    
    // –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º input –≤ FP16 –∏ –æ–±—Ä–∞—Ç–Ω–æ
    let input_f16 = input.to_f16()?;
    let input_restored = input_f16.to_f32()?;
    
    // Forward pass —Å –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–º input
    let output_restored = linear.forward(&input_restored)?;
    println!("  Output restored dtype: {:?}", output_restored.dtype());
    
    // –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    let diff = output_f32.sub(&output_restored)?;
    let max_diff: f32 = diff.data().iter().fold(0.0, |acc, &x| acc.max(x.abs()));
    println!("  Max difference between FP32 and FP16->FP32: {:.8}", max_diff);
    
    // 10. –ü—Ä–æ–≤–µ—Ä–∫–∞ –ø–æ–¥–¥–µ—Ä–∂–∫–∏ mixed precision
    println!("\n10. Mixed Precision Support:");
    println!("  Tensor supports mixed precision: {}", tensor_f32.supports_mixed_precision());
    println!("  FP16 tensor supports mixed precision: {}", tensor_f16.supports_mixed_precision());
    println!("  INT8 tensor supports mixed precision: {}", tensor_i8.supports_mixed_precision());
    
    println!("\n‚úÖ Mixed precision example completed successfully!");
    println!("   FP16/INT8 support is now available!");
    
    Ok(())
}
